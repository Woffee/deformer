[DEFAULT]
model = bert
task =
use_replace_map = yes

;data_dir = gs://bert-gcs/deformer
data_dir = /Users/woffee/www/emse-apiqa/baselines/deformer/data
# can export inference graph if run in session/graph mode
inference_graph = ${data_dir}/ckpt/${model}/${task}_${model}_infer.pb


### experiments control params ###
svd_units = 0
attn_drop_percentile = 0.0
attn_drop_renormalize = no
# row or col
attn_drop_type = 'row'
attn_profile = no
# min or max
attn_col_type = 'min'
tune_scopes =


### resource params ###
;checkpoint_dir = ${data_dir}/ckpt/${model}/${task}2
checkpoint_dir = ${data_dir}/ckpt/${model}-base/${task}
vocab_file = ${data_dir}/res/bert.vocab
lower_case = yes
context_stride = 128


### core parameters ###
max_answer_span = 30
max_seq_length =
# 2 for squad, 3 for hotpot
num_classes = 2

attention_dropout_prob = 0.1
intermediate_act_fn = gelu
hidden_dropout_prob = 0.1
hidden_size = 768
initializer_range = 0.02
intermediate_size = 3072
max_position_embeddings = 512
num_heads = 12
attention_head_size = 64
num_hidden_layers = 12
type_vocab_size = 2
vocab_size = 30522


### train, dev, tune or analyze files ###
mode = train
dataset_file = ${data_dir}/datasets/converted/${model}/${task}-${mode}.*.tfrecord
# optional total number of examples, if not give, will automatically
# extract from dataset_file name extension
dataset_size =
ground_truth_file = ${data_dir}/datasets/converted/${model}/${task}-dev.*.jsonl
output_file = ${data_dir}/predictions/${model}/${task}-dev-predictions.json


### train params ###
keep_checkpoint_max = 20
epochs = 3
random_seed = 0
learning_rate = 5e-5
warmup_ratio = 0.15
steps_per_checkpoint = 1000
init_checkpoint = ${checkpoint_dir}
; init_checkpoint = ${data_dir}/ckpt/init/uncased_base/bert_model.ckpt
;init_checkpoint = gs://bert-gcs/bert_pretrained/base/bert_model.ckpt
;init_checkpoint = ${data_dir}/ckpt/${model}/${task}/svd_${svd_units}
;init_checkpoint = gs://bert-gcs/deformer/init/bert/qa_p

### input params ###
train_batch_size = 32
dev_batch_size = 32
input_num_threads = 8
input_buffer_size = 2000
iterations_per_loop = 1000


### tpu params ###
use_tpu = no
# required if use tpu, the Cloud TPU name, or an URL like grpc://ip.address.of.tpu:8470
tpu_name =
# optional, GCE zone where the Cloud TPU is located in
;tpu_zone =
# optional, project name for the Cloud TPU-enabled project
;gcp_project =
# optional, TensorFlow master URL
;master =
# if use tpu, number of TPU cores to use
num_tpu_cores = 8
bfloat16 =


# only effective for GPU and CPU
optimize_padding = no

### debug params
debug = no
debug_save_dir = ${data_dir}/debug_info
print_steps = 100
